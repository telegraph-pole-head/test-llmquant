{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Test of Llama and OPT on vllm\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "set up the virtual environment with the following commands:\n",
    "```bash\n",
    "conda create -n vllm python=3.9\n",
    "conda activate vllm\n",
    "python -m pip install vllm=0.5.1\n",
    "# python -m pip install torchvision==0.18.0 #(or) other vision correspond to your torch.__vision__\n",
    "# better in another dir:\n",
    "git clone https://github.com/EleutherAI/lm-evaluation-harness # (use kkgithub or gitee if can't access github)\n",
    "cd lm-evaluation-harness\n",
    "python -m pip install -e .\"vllm\"\n",
    "python -m pip install lm_eval[vllm]\n",
    "# Optional: using wandb (and weave) for logging and visualization\n",
    "python -m pip install weave llmuses lm_eval[wandb]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model download\n",
    "\n",
    "put the following at the end of your `~/.bashrc` file:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Settings for Hugging Face\n",
    "export HF_ENDPOINT=https://hf-mirror.com # use the mirror\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 # use hf transfer to download models faster\n",
    "\n",
    "```\n",
    "or, you can (uncomment and) run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"export HF_ENDPOINT=https://hf-mirror.com\" >> ~/.bashrc\n",
    "# echo \"export HF_HUB_ENABLE_HF_TRANSFER=1\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, (uncomment and) run the following command to download the LLaMA-7b, Llama-2-7b and opt-1.3b, opt-2.7b model:\n",
    "- [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)\n",
    "- [TheBloke/Llama-2-7B-fp16](https://huggingface.co/TheBloke/Llama-2-7B-fp16/tree/refs%2Fpr%2F6)\n",
    "- [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)\n",
    "- [facebook/opt-2.7b](https://huggingface.co/facebook/opt-12.7b)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download huggyllama/llama-7b --local-dir /data/llmQuantModels/LLaMA-7b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download TheBloke/Llama-2-7B-fp16 --local-dir /data/llmQuantModels/Llama-2-7b --revision refs/pr/6\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download facebook/opt-1.3b --local-dir /data/llmQuantModels/opt-1.3b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download facebook/opt-2.7b --local-dir /data/llmQuantModels/opt-2.7b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download microsoft/Phi-3-mini-4k-instruct --local-dir /data/llmQuantModels/phi-3-mini\n",
    "```\n",
    "\n",
    "\n",
    "For the quiantized visions:\n",
    "\n",
    "- [OmniQuant](https://huggingface.co/ChenMnZ/OmniQuant/tree/main)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download ChenMnZ/OmniQuant Llama-2-7b-w4a16g128.pth --local-dir /data/llmQuantModels/Llama-2-7b-w4a16g128/\n",
    "cd utils\n",
    "python safetensor_converter.py /data/llmQuantModels/Llama-2-7b-w4a16g128/\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- [TheBloke/LLaMa-7B-GPTQ](https://huggingface.co/TheBloke/LLaMa-7B-GPTQ)\n",
    "- [TheBloke/LLaMa-7B-GGML](https://huggingface.co/TheBloke/LLaMa-7B-GGML)\n",
    "- [TheBloke/Llama-2-7B-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-GGUF)\n",
    "- [TheBloke/Llama-2-7B-GGML](https://huggingface.co/TheBloke/Llama-2-7B-GGML)\n",
    "- [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ)\n",
    "- [TheBloke/Llama-2-7B-AWQ](https://huggingface.co/TheBloke/Llama-2-7B-AWQ)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download TheBloke/Llama-2-7B-AWQ --local-dir /data/llmQuantModels/Llama-2-7B-AWQ\n",
    "```\n",
    "\n",
    "For further compression:\n",
    "- [PowerInfer/prosparse-llama-2-7b-gguf](https://huggingface.co/PowerInfer/prosparse-llama-2-7b-gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import utils\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "import wandb\n",
    "\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get LLaMA model, for example\n",
    "# model = LLM(\"/data/llmQuantModels/Llama-2-7b\")\n",
    "# if want to use 2 GPUs:\n",
    "\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
    "\n",
    "model = LLM(\n",
    "    \"/data/llmQuantModels/gemma-2-9b-it-GPTQ\",\n",
    "    # tensor_parallel_size=2,\n",
    "    trust_remote_code=True,\n",
    "    quantization=\"gptq\",\n",
    "    gpu_memory_utilization=0.45,\n",
    "    dtype=torch.float16,\n",
    "    # max_model_len=2048,\n",
    "    enforce_eager=True,\n",
    "    # load_format=\"bitsandbytes\",\n",
    ")\n",
    "# , gpu_memory_utilization=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 55.07 toks/s, output: 35.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=4, prompt='<bos><start_of_turn>user\\nwhich is bigger, 9.9 or 9.11?<end_of_turn>\\n<start_of_turn>model\\n', prompt_token_ids=[2, 2, 106, 1645, 108, 7769, 603, 14739, 235269, 235248, 235315, 235265, 235315, 689, 235248, 235315, 235265, 235274, 235274, 235336, 107, 108, 106, 2516, 108], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9.9 is bigger than 9.11. \\n', token_ids=(235315, 235265, 235315, 603, 14739, 1178, 235248, 235315, 235265, 235274, 235274, 235265, 235248, 108, 107, 1), cumulative_logprob=-0.33067090436816216, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1721714817.44014, last_token_time=1721714817.44014, first_scheduled_time=1721714817.4425955, first_token_time=1721714817.4810452, time_in_queue=0.002455472946166992, finished_time=1721714817.896371), lora_request=None)]\n",
      "Prompt: '<bos><start_of_turn>user\\nwhich is bigger, 9.9 or 9.11?<end_of_turn>\\n<start_of_turn>model\\n',\n",
      "Generated text: '9.9 is bigger than 9.11. \\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/llmQuantModels/gemma-2-9b-it\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"which is bigger, 9.9 or 9.11?\",\n",
    "    }\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():  # reduce useless gradient computation\n",
    "    outputs = model.generate(formatted_prompt, sampling_params)\n",
    "\n",
    "print(outputs)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r},\\nGenerated text: {generated_text!r}\")\n",
    "\n",
    "    data = {\n",
    "        \"arrival_time\": output.metrics.arrival_time,\n",
    "        \"last_token_time\": output.metrics.last_token_time,\n",
    "        \"first_scheduled_time\": output.metrics.first_scheduled_time,\n",
    "        \"first_token_time\": output.metrics.first_token_time,\n",
    "        \"time_in_queue\": output.metrics.time_in_queue,\n",
    "        \"finished_time\": output.metrics.finished_time,\n",
    "    }\n",
    "    # utils.plot.show_timeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage\n",
    "\n",
    "meanwhile, the memory use can also be monitored by the following command:\n",
    "\n",
    "```bash\n",
    "watch -n 1 nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated on CUDA device: 16.08598518371582\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16472 MiB |  19380 MiB |   1806 GiB |   1790 GiB |\n",
      "|       from large pool |  16470 MiB |  19378 MiB |   1800 GiB |   1784 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16472 MiB |  19380 MiB |   1806 GiB |   1790 GiB |\n",
      "|       from large pool |  16470 MiB |  19378 MiB |   1800 GiB |   1784 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16472 MiB |  19379 MiB |   1803 GiB |   1787 GiB |\n",
      "|       from large pool |  16470 MiB |  19377 MiB |   1798 GiB |   1782 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  20280 MiB |  20280 MiB |  21044 MiB | 782336 KiB |\n",
      "|       from large pool |  20266 MiB |  20266 MiB |  21002 MiB | 753664 KiB |\n",
      "|       from small pool |     14 MiB |     14 MiB |     42 MiB |  28672 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  85966 KiB |   1330 MiB |   1123 GiB |   1123 GiB |\n",
      "|       from large pool |  83712 KiB |   1328 MiB |   1112 GiB |   1112 GiB |\n",
      "|       from small pool |   2254 KiB |      5 MiB |     11 GiB |     11 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     306    |     342    |   66644    |   66338    |\n",
      "|       from large pool |     165    |     172    |   40957    |   40792    |\n",
      "|       from small pool |     141    |     174    |   25687    |   25546    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     306    |     342    |   66644    |   66338    |\n",
      "|       from large pool |     165    |     172    |   40957    |   40792    |\n",
      "|       from small pool |     141    |     174    |   25687    |   25546    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     188    |     188    |     225    |      37    |\n",
      "|       from large pool |     181    |     181    |     204    |      23    |\n",
      "|       from small pool |       7    |       7    |      21    |      14    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      39    |      47    |   34880    |   34841    |\n",
      "|       from large pool |      37    |      45    |   26271    |   26234    |\n",
      "|       from small pool |       2    |       7    |    8609    |    8607    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\n",
    "    \"Memory allocated on CUDA device:\",\n",
    "    torch.cuda.memory_allocated() / 1024 / 1024 / 1024,\n",
    ")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "# try:\n",
    "#     del model.llm_engine.model_executor\n",
    "#     del model\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Harness\n",
    "\n",
    "### Basic Examples\n",
    "Example test:\n",
    "```bash\n",
    "lm_eval --model vllm \\\n",
    "    --model_args pretrained=\"/data/llmQuantModels/Llama-2-7b\",tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7\\\n",
    "    --tasks lambada_openai \\\n",
    "    --batch_size auto\n",
    "```\n",
    "results:\n",
    "vllm (pretrained=./models/Llama-2,tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
    "|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value |   |Stderr|\n",
    "|--------------|------:|------|-----:|----------|---|-----:|---|-----:|\n",
    "|lambada_openai|      1|none  |     0|acc       |↑  |0.7407|±  |0.0061|\n",
    "|              |       |none  |     0|perplexity|↓  |3.3953|±  |0.0669|\n",
    "\n",
    "Example test:\n",
    "```bash\n",
    "lm_eval --model vllm \\\n",
    "    --model_args pretrained=\"/data/llmQuantModels/Llama-2-7b\",tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7\\\n",
    "    --tasks wikitext\\\n",
    "    --batch_size auto\n",
    "```\n",
    "results:\n",
    "vllm (pretrained=./models/Llama-2,tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
    "| Tasks  |Version|Filter|n-shot|    Metric     |   |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|---------------|---|-----:|---|------|\n",
    "|wikitext|      2|none  |     0|bits_per_byte  |↓  |0.5866|±  |N/A   |\n",
    "|        |       |none  |     0|byte_perplexity|↓  |1.5017|±  |N/A   |\n",
    "|        |       |none  |     0|word_perplexity|↓  |8.7942|±  |N/A   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-24 21:22:01 config.py:1354] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-24 21:22:01 gptq_marlin.py:145] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "WARNING 07-24 21:22:01 config.py:244] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-24 21:22:01 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='/data/llmQuantModels/gemma-2-9b-it-GPTQ', speculative_config=None, tokenizer='/data/llmQuantModels/gemma-2-9b-it-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=1234, served_model_name=/data/llmQuantModels/gemma-2-9b-it-GPTQ, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVLLM_ATTENTION_BACKEND\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLASHINFER\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# torch.cuda.set_device(1)\\\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"basic_vllm.ipynb\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menforce_eager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device=\"cuda:1\",\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu_memory_utilization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_model_len=2048,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_dtye\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# kv_cache_dtype=\"fp8\",\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_wandb=False,\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my/test/utils/eval.py:76\u001b[0m, in \u001b[0;36meval\u001b[0;34m(platform, model_name, tensor_parallel, infer_dtye, gpu_memory_utilization, max_model_len, enforce_eager, batch_size, tasks, log_samples, use_wandb, del_logs, device, quantization, kv_cache_dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m llm_eval_err \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(llm_eval_out), redirect_stderr(\n\u001b[1;32m     74\u001b[0m     llm_eval_err\n\u001b[1;32m     75\u001b[0m ):  \u001b[38;5;66;03m# Redirect the stdout and stderr to the io.StringIO object\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mlm_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplatform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Analyze the log\u001b[39;00m\n\u001b[1;32m     85\u001b[0m log_out \u001b[38;5;241m=\u001b[39m llm_eval_out\u001b[38;5;241m.\u001b[39mgetvalue() \u001b[38;5;241m+\u001b[39m llm_eval_err\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/lm_eval/utils.py:395\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/lm_eval/evaluator.py:192\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, max_batch_size, device, use_cache, cache_requests, rewrite_requests_cache, delete_requests_cache, limit, bootstrap_iters, check_integrity, write_out, log_samples, evaluation_tracker, system_instruction, apply_chat_template, fewshot_as_multiturn, gen_kwargs, task_manager, verbosity, predict_only, random_seed, numpy_random_seed, torch_random_seed, fewshot_random_seed)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         eval_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model, with arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimple_parse_args_string(model_args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0m         lm \u001b[38;5;241m=\u001b[39m \u001b[43mlm_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_arg_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, lm_eval\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mLM):\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/lm_eval/api/model.py:148\u001b[0m, in \u001b[0;36mLM.create_from_arg_string\u001b[0;34m(cls, arg_string, additional_config)\u001b[0m\n\u001b[1;32m    146\u001b[0m args \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msimple_parse_args_string(arg_string)\n\u001b[1;32m    147\u001b[0m args2 \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m additional_config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py:103\u001b[0m, in \u001b[0;36mVLLM.__init__\u001b[0;34m(self, pretrained, dtype, revision, trust_remote_code, tokenizer, tokenizer_mode, tokenizer_revision, add_bos_token, prefix_token_id, tensor_parallel_size, quantization, max_gen_toks, swap_space, batch_size, max_batch_size, max_length, max_model_len, seed, gpu_memory_utilization, device, data_parallel_size, lora_local_path, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch_size\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batch_size\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     eval_logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou might experience occasional issues with model weight downloading when data_parallel is in use. To ensure stable performance, run with data_parallel_size=1 until the weights are downloaded and cached.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:149\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    130\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    131\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:414\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    411\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:243\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    238\u001b[0m     model_config)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m INPUT_REGISTRY\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/executor/executor_base.py:42\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultimodal_config \u001b[38;5;241m=\u001b[39m multimodal_config\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:24\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/worker/worker.py:133\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/worker/model_runner.py:243\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    255\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    256\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:21\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, multimodal_config, cache_config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     15\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     16\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     17\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     18\u001b[0m               multimodal_config: Optional[MultiModalConfig],\n\u001b[1;32m     19\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     20\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:267\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, multimodal_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device_config\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m--> 267\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_iterator(model_config\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    272\u001b[0m                                    model_config\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall_back_to_pt_during_load\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    276\u001b[0m                                        \u001b[38;5;28;01mTrue\u001b[39;00m)), )\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:104\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, multimodal_config, cache_config)\u001b[0m\n\u001b[1;32m    101\u001b[0m model_class \u001b[38;5;241m=\u001b[39m get_model_architecture(model_config)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    102\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m _get_quantization_config(model_config, load_config)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_get_model_initialization_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:323\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mGemma2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits_processor \u001b[38;5;241m=\u001b[39m LogitsProcessor(\n\u001b[1;32m    325\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size, soft_cap\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfinal_logit_softcapping)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m Sampler()\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:251\u001b[0m, in \u001b[0;36mGemma2Model.__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    248\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    249\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    250\u001b[0m )\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    252\u001b[0m     Gemma2DecoderLayer(layer_idx, config, cache_config, quant_config)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    254\u001b[0m ])\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m GemmaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Normalize the embedding by sqrt(hidden_size)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# The normalizer's data type should be downcasted to the model's\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# data type such as bfloat16, not float32.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    248\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    249\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 252\u001b[0m     \u001b[43mGemma2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    254\u001b[0m ])\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m GemmaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Normalize the embedding by sqrt(hidden_size)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# The normalizer's data type should be downcasted to the model's\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# data type such as bfloat16, not float32.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:178\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.__init__\u001b[0;34m(self, layer_idx, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mGemma2Attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m Gemma2MLP(\n\u001b[1;32m    192\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    193\u001b[0m     intermediate_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mintermediate_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    197\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:130\u001b[0m, in \u001b[0;36mGemma2Attention.__init__\u001b[0;34m(self, layer_idx, config, hidden_size, num_heads, num_kv_heads, head_dim, max_position_embeddings, rope_theta, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_num_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    125\u001b[0m     hidden_size,\n\u001b[1;32m    126\u001b[0m     bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias,\n\u001b[1;32m    127\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# TODO(woosuk): Use the `get_rope` interface.\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mGemmaRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_neox_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# FIXME(woosuk): While Gemma 2 uses sliding window attention for every\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# odd layer, vLLM currently ignores it and uses global attention for\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# all layers.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m use_sliding_window \u001b[38;5;241m=\u001b[39m (layer_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    143\u001b[0m                       \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/layers/rotary_embedding.py:80\u001b[0m, in \u001b[0;36mRotaryEmbedding.__init__\u001b[0;34m(self, head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_neox_style \u001b[38;5;241m=\u001b[39m is_neox_style\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m---> 80\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_cos_sin_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_native2 \u001b[38;5;241m=\u001b[39m is_tpu() \u001b[38;5;129;01mand\u001b[39;00m is_neox_style\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_native2:\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/vllm/model_executor/layers/rotary_embedding.py:112\u001b[0m, in \u001b[0;36mRotaryEmbedding._compute_cos_sin_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m freqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi,j -> ij\u001b[39m\u001b[38;5;124m\"\u001b[39m, t, inv_freq)\n\u001b[1;32m    111\u001b[0m cos \u001b[38;5;241m=\u001b[39m freqs\u001b[38;5;241m.\u001b[39mcos()\n\u001b[0;32m--> 112\u001b[0m sin \u001b[38;5;241m=\u001b[39m \u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cos, sin), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache\n",
      "File \u001b[0;32m~/.conda/envs/nvllm/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "from utils import eval\n",
    "import os\n",
    "import time\n",
    "import torch  # type: ignore\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(2)\n",
    "\n",
    "sets = [\n",
    "    {\"task\": \"mmlu\", \"enforce_eager\": False, \"gpu_memory_utilization\": 0.6},\n",
    "    {\"task\": \"wikitext\", \"enforce_eager\": True, \"gpu_memory_utilization\": 0.45},\n",
    "]\n",
    "\n",
    "set_0 = sets[1]\n",
    "model_name = \"gemma-2-9b-it-GPTQ\"\n",
    "quantization = \"GPTQ\"\n",
    "\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
    "# torch.cuda.set_device(1)\\\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"basic_vllm.ipynb\"\n",
    "\n",
    "res = eval.eval(\n",
    "    model_name=model_name,\n",
    "    enforce_eager=set_0[\"enforce_eager\"],\n",
    "    tensor_parallel=False,\n",
    "    tasks=set_0[\"task\"],\n",
    "    # device=\"cuda:1\",\n",
    "    gpu_memory_utilization=set_0[\"gpu_memory_utilization\"],\n",
    "    # max_model_len=2048,\n",
    "    quantization=quantization,\n",
    "    infer_dtye=\"float16\",\n",
    "    # kv_cache_dtype=\"fp8\",\n",
    "    # use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'gsm8k': {'alias': 'gsm8k',\n",
       "   'exact_match,strict-match': 0.7065959059893859,\n",
       "   'exact_match_stderr,strict-match': 0.01254183081546149,\n",
       "   'exact_match,flexible-extract': 0.7210007581501138,\n",
       "   'exact_match_stderr,flexible-extract': 0.012354115779970322}},\n",
       " 'group_subtasks': {'gsm8k': []},\n",
       " 'configs': {'gsm8k': {'task': 'gsm8k',\n",
       "   'tag': ['math_word_problems'],\n",
       "   'dataset_path': 'gsm8k',\n",
       "   'dataset_name': 'main',\n",
       "   'training_split': 'train',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'train',\n",
       "   'doc_to_text': 'Question: {{question}}\\nAnswer:',\n",
       "   'doc_to_target': '{{answer}}',\n",
       "   'description': '',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'num_fewshot': 5,\n",
       "   'metric_list': [{'metric': 'exact_match',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True,\n",
       "     'ignore_case': True,\n",
       "     'ignore_punctuation': False,\n",
       "     'regexes_to_ignore': [',', '\\\\$', '(?s).*#### ', '\\\\.$']}],\n",
       "   'output_type': 'generate_until',\n",
       "   'generation_kwargs': {'until': ['Question:', '</s>', '<|im_end|>'],\n",
       "    'do_sample': False,\n",
       "    'temperature': 0.0},\n",
       "   'repeats': 1,\n",
       "   'filter_list': [{'name': 'strict-match',\n",
       "     'filter': [{'function': 'regex',\n",
       "       'regex_pattern': '#### (\\\\-?[0-9\\\\.\\\\,]+)'},\n",
       "      {'function': 'take_first'}]},\n",
       "    {'name': 'flexible-extract',\n",
       "     'filter': [{'function': 'regex',\n",
       "       'group_select': -1,\n",
       "       'regex_pattern': '(-?[$0-9.,]{2,})|(-?[0-9]+)'},\n",
       "      {'function': 'take_first'}]}],\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 3.0}}},\n",
       " 'versions': {'gsm8k': 3.0},\n",
       " 'n-shot': {'gsm8k': 5},\n",
       " 'higher_is_better': {'gsm8k': {'exact_match': True}},\n",
       " 'n-samples': {'gsm8k': {'original': 1319, 'effective': 1319}},\n",
       " 'config': {'model': 'vllm',\n",
       "  'model_args': 'pretrained=/data/llmQuantModels/phi-3-mini-AWQ,dtype=auto,gpu_memory_utilization=0.4,batch_size=auto,enforce_eager=True,quantization=AWQ,',\n",
       "  'batch_size': None,\n",
       "  'batch_sizes': [],\n",
       "  'device': None,\n",
       "  'use_cache': None,\n",
       "  'limit': None,\n",
       "  'bootstrap_iters': 100000,\n",
       "  'gen_kwargs': None,\n",
       "  'random_seed': 0,\n",
       "  'numpy_seed': 1234,\n",
       "  'torch_seed': 1234,\n",
       "  'fewshot_seed': 1234},\n",
       " 'git_hash': '1e52658',\n",
       " 'date': 1721500413.0731828,\n",
       " 'pretty_env_info': 'PyTorch version: 2.3.0+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 20.04.6 LTS (x86_64)\\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\\nClang version: 10.0.0-4ubuntu1 \\nCMake version: version 3.30.0\\nLibc version: glibc-2.31\\n\\nPython version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0] (64-bit runtime)\\nPython platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31\\nIs CUDA available: True\\nCUDA runtime version: 12.1.66\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: \\nGPU 0: NVIDIA GeForce RTX 4090\\nGPU 1: NVIDIA GeForce RTX 4090\\n\\nNvidia driver version: 535.129.03\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.1\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.7\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nByte Order:                         Little Endian\\nAddress sizes:                      46 bits physical, 57 bits virtual\\nCPU(s):                             64\\nOn-line CPU(s) list:                0-63\\nThread(s) per core:                 1\\nCore(s) per socket:                 32\\nSocket(s):                          2\\nNUMA node(s):                       2\\nVendor ID:                          GenuineIntel\\nCPU family:                         6\\nModel:                              106\\nModel name:                         Intel(R) Xeon(R) Platinum 8338C CPU @ 2.60GHz\\nStepping:                           6\\nCPU MHz:                            800.000\\nCPU max MHz:                        3500.0000\\nCPU min MHz:                        800.0000\\nBogoMIPS:                           5200.00\\nVirtualization:                     VT-x\\nL1d cache:                          3 MiB\\nL1i cache:                          2 MiB\\nL2 cache:                           80 MiB\\nL3 cache:                           96 MiB\\nNUMA node0 CPU(s):                  0-31\\nNUMA node1 CPU(s):                  32-63\\nVulnerability Gather data sampling: Mitigation; Microcode\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Not affected\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT disabled\\nVulnerability Retbleed:             Not affected\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Not affected\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.26.4\\n[pip3] torch==2.3.0\\n[pip3] torchvision==0.18.0\\n[pip3] triton==2.3.0\\n[conda] numpy                     1.26.4                   pypi_0    pypi\\n[conda] torch                     2.3.0                    pypi_0    pypi\\n[conda] torchvision               0.18.0                   pypi_0    pypi\\n[conda] triton                    2.3.0                    pypi_0    pypi',\n",
       " 'transformers_version': '4.42.4',\n",
       " 'upper_git_hash': None,\n",
       " 'tokenizer_pad_token': ['<|endoftext|>', '32000'],\n",
       " 'tokenizer_eos_token': ['<|endoftext|>', '32000'],\n",
       " 'tokenizer_bos_token': ['<s>', '1'],\n",
       " 'eot_token_id': 32000,\n",
       " 'max_length': 4096,\n",
       " 'model_size': None,\n",
       " 'avg_output_toks': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 4090\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "    torch.cuda.get_device_properties(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.944580078125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(16972316672 + 24844107776) / 1024 / 1024 / 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
