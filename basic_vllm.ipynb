{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Test of Llama and OPT on vllm\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "set up the virtual environment with the following commands:\n",
    "```bash\n",
    "conda create -n vllm python=3.9\n",
    "conda activate vllm\n",
    "python -m pip install vllm=0.5.1\n",
    "# python -m pip install torchvision==0.18.0 #(or) other vision correspond to your torch.__vision__\n",
    "# better in another dir:\n",
    "git clone https://github.com/EleutherAI/lm-evaluation-harness # (use kkgithub or gitee if can't access github)\n",
    "cd lm-evaluation-harness\n",
    "python -m pip install -e .\"vllm\"\n",
    "python -m pip install lm_eval[vllm]\n",
    "# Optional: using wandb (and weave) for logging and visualization\n",
    "python -m pip install weave llmuses lm_eval[wandb]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model download\n",
    "\n",
    "put the following at the end of your `~/.bashrc` file:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Settings for Hugging Face\n",
    "export HF_ENDPOINT=https://hf-mirror.com # use the mirror\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 # use hf transfer to download models faster\n",
    "\n",
    "```\n",
    "or, you can (uncomment and) run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"export HF_ENDPOINT=https://hf-mirror.com\" >> ~/.bashrc\n",
    "# echo \"export HF_HUB_ENABLE_HF_TRANSFER=1\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, (uncomment and) run the following command to download the LLaMA-7b, Llama-2-7b and opt-1.3b, opt-2.7b model:\n",
    "- [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)\n",
    "- [TheBloke/Llama-2-7B-fp16](https://huggingface.co/TheBloke/Llama-2-7B-fp16/tree/refs%2Fpr%2F6)\n",
    "- [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)\n",
    "- [facebook/opt-2.7b](https://huggingface.co/facebook/opt-12.7b)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download huggyllama/llama-7b --local-dir /data/llmQuantModels/LLaMA-7b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download TheBloke/Llama-2-7B-fp16 --local-dir /data/llmQuantModels/Llama-2-7b --revision refs/pr/6\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download facebook/opt-1.3b --local-dir /data/llmQuantModels/opt-1.3b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download facebook/opt-2.7b --local-dir /data/llmQuantModels/opt-2.7b\n",
    "```\n",
    "\n",
    "```bash\n",
    "huggingface-cli download microsoft/Phi-3-mini-4k-instruct --local-dir /data/llmQuantModels/phi-3-mini\n",
    "```\n",
    "\n",
    "\n",
    "For the quiantized visions:\n",
    "\n",
    "- [OmniQuant](https://huggingface.co/ChenMnZ/OmniQuant/tree/main)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download ChenMnZ/OmniQuant Llama-2-7b-w4a16g128.pth --local-dir /data/llmQuantModels/Llama-2-7b-w4a16g128/\n",
    "cd utils\n",
    "python safetensor_converter.py /data/llmQuantModels/Llama-2-7b-w4a16g128/\n",
    "\n",
    "```\n",
    "huggingface-cli download TheBloke/Llama-2-13B-GPTQ --local-dir /data/llmQuantModels/Llama-2-13B-GPTQ-8bit --revision gptq-8bit-128g-actorder_True\n",
    "\n",
    "- [TheBloke/LLaMa-7B-GPTQ](https://huggingface.co/TheBloke/LLaMa-7B-GPTQ)\n",
    "- [TheBloke/LLaMa-7B-GGML](https://huggingface.co/TheBloke/LLaMa-7B-GGML)\n",
    "- [TheBloke/Llama-2-7B-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-GGUF)\n",
    "- [TheBloke/Llama-2-7B-GGML](https://huggingface.co/TheBloke/Llama-2-7B-GGML)\n",
    "- [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ)\n",
    "- [TheBloke/Llama-2-7B-AWQ](https://huggingface.co/TheBloke/Llama-2-7B-AWQ)\n",
    "\n",
    "```bash\n",
    "huggingface-cli download TheBloke/Llama-2-7B-AWQ --local-dir /data/llmQuantModels/Llama-2-7B-AWQ\n",
    "```\n",
    "\n",
    "For further compression:\n",
    "- [PowerInfer/prosparse-llama-2-7b-gguf](https://huggingface.co/PowerInfer/prosparse-llama-2-7b-gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import utils\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "import wandb\n",
    "\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get LLaMA model, for example\n",
    "# model = LLM(\"/data/llmQuantModels/Llama-2-7b\")\n",
    "# if want to use 2 GPUs:\n",
    "\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
    "\n",
    "model = LLM(\n",
    "    \"/data/llmQuantModels/gemma-2-9b-it-GPTQ\",\n",
    "    # tensor_parallel_size=2,\n",
    "    trust_remote_code=True,\n",
    "    quantization=\"gptq\",\n",
    "    gpu_memory_utilization=0.45,\n",
    "    dtype=torch.float16,\n",
    "    # max_model_len=2048,\n",
    "    enforce_eager=True,\n",
    "    # load_format=\"bitsandbytes\",\n",
    ")\n",
    "# , gpu_memory_utilization=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s, est. speed input: 55.07 toks/s, output: 35.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=4, prompt='<bos><start_of_turn>user\\nwhich is bigger, 9.9 or 9.11?<end_of_turn>\\n<start_of_turn>model\\n', prompt_token_ids=[2, 2, 106, 1645, 108, 7769, 603, 14739, 235269, 235248, 235315, 235265, 235315, 689, 235248, 235315, 235265, 235274, 235274, 235336, 107, 108, 106, 2516, 108], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9.9 is bigger than 9.11. \\n', token_ids=(235315, 235265, 235315, 603, 14739, 1178, 235248, 235315, 235265, 235274, 235274, 235265, 235248, 108, 107, 1), cumulative_logprob=-0.33067090436816216, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1721714817.44014, last_token_time=1721714817.44014, first_scheduled_time=1721714817.4425955, first_token_time=1721714817.4810452, time_in_queue=0.002455472946166992, finished_time=1721714817.896371), lora_request=None)]\n",
      "Prompt: '<bos><start_of_turn>user\\nwhich is bigger, 9.9 or 9.11?<end_of_turn>\\n<start_of_turn>model\\n',\n",
      "Generated text: '9.9 is bigger than 9.11. \\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/llmQuantModels/gemma-2-9b-it\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"which is bigger, 9.9 or 9.11?\",\n",
    "    }\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():  # reduce useless gradient computation\n",
    "    outputs = model.generate(formatted_prompt, sampling_params)\n",
    "\n",
    "print(outputs)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r},\\nGenerated text: {generated_text!r}\")\n",
    "\n",
    "    data = {\n",
    "        \"arrival_time\": output.metrics.arrival_time,\n",
    "        \"last_token_time\": output.metrics.last_token_time,\n",
    "        \"first_scheduled_time\": output.metrics.first_scheduled_time,\n",
    "        \"first_token_time\": output.metrics.first_token_time,\n",
    "        \"time_in_queue\": output.metrics.time_in_queue,\n",
    "        \"finished_time\": output.metrics.finished_time,\n",
    "    }\n",
    "    # utils.plot.show_timeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage\n",
    "\n",
    "meanwhile, the memory use can also be monitored by the following command:\n",
    "\n",
    "```bash\n",
    "watch -n 1 nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated on CUDA device: 16.08598518371582\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16472 MiB |  19380 MiB |   1806 GiB |   1790 GiB |\n",
      "|       from large pool |  16470 MiB |  19378 MiB |   1800 GiB |   1784 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16472 MiB |  19380 MiB |   1806 GiB |   1790 GiB |\n",
      "|       from large pool |  16470 MiB |  19378 MiB |   1800 GiB |   1784 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16472 MiB |  19379 MiB |   1803 GiB |   1787 GiB |\n",
      "|       from large pool |  16470 MiB |  19377 MiB |   1798 GiB |   1782 GiB |\n",
      "|       from small pool |      1 MiB |      5 MiB |      5 GiB |      5 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  20280 MiB |  20280 MiB |  21044 MiB | 782336 KiB |\n",
      "|       from large pool |  20266 MiB |  20266 MiB |  21002 MiB | 753664 KiB |\n",
      "|       from small pool |     14 MiB |     14 MiB |     42 MiB |  28672 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  85966 KiB |   1330 MiB |   1123 GiB |   1123 GiB |\n",
      "|       from large pool |  83712 KiB |   1328 MiB |   1112 GiB |   1112 GiB |\n",
      "|       from small pool |   2254 KiB |      5 MiB |     11 GiB |     11 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     306    |     342    |   66644    |   66338    |\n",
      "|       from large pool |     165    |     172    |   40957    |   40792    |\n",
      "|       from small pool |     141    |     174    |   25687    |   25546    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     306    |     342    |   66644    |   66338    |\n",
      "|       from large pool |     165    |     172    |   40957    |   40792    |\n",
      "|       from small pool |     141    |     174    |   25687    |   25546    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     188    |     188    |     225    |      37    |\n",
      "|       from large pool |     181    |     181    |     204    |      23    |\n",
      "|       from small pool |       7    |       7    |      21    |      14    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      39    |      47    |   34880    |   34841    |\n",
      "|       from large pool |      37    |      45    |   26271    |   26234    |\n",
      "|       from small pool |       2    |       7    |    8609    |    8607    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\n",
    "    \"Memory allocated on CUDA device:\",\n",
    "    torch.cuda.memory_allocated() / 1024 / 1024 / 1024,\n",
    ")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "# try:\n",
    "#     del model.llm_engine.model_executor\n",
    "#     del model\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Harness\n",
    "\n",
    "### Basic Examples\n",
    "Example test:\n",
    "```bash\n",
    "lm_eval --model vllm \\\n",
    "    --model_args pretrained=\"/data/llmQuantModels/Llama-2-7b\",tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7\\\n",
    "    --tasks lambada_openai \\\n",
    "    --batch_size auto\n",
    "```\n",
    "results:\n",
    "vllm (pretrained=./models/Llama-2,tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
    "|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value |   |Stderr|\n",
    "|--------------|------:|------|-----:|----------|---|-----:|---|-----:|\n",
    "|lambada_openai|      1|none  |     0|acc       |↑  |0.7407|±  |0.0061|\n",
    "|              |       |none  |     0|perplexity|↓  |3.3953|±  |0.0669|\n",
    "\n",
    "Example test:\n",
    "```bash\n",
    "lm_eval --model vllm \\\n",
    "    --model_args pretrained=\"/data/llmQuantModels/Llama-2-7b\",tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7\\\n",
    "    --tasks wikitext\\\n",
    "    --batch_size auto\n",
    "```\n",
    "results:\n",
    "vllm (pretrained=./models/Llama-2,tensor_parallel_size=2,dtype=auto,gpu_memory_utilization=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
    "| Tasks  |Version|Filter|n-shot|    Metric     |   |Value |   |Stderr|\n",
    "|--------|------:|------|-----:|---------------|---|-----:|---|------|\n",
    "|wikitext|      2|none  |     0|bits_per_byte  |↓  |0.5866|±  |N/A   |\n",
    "|        |       |none  |     0|byte_perplexity|↓  |1.5017|±  |N/A   |\n",
    "|        |       |none  |     0|word_perplexity|↓  |8.7942|±  |N/A   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval\n",
    "import os\n",
    "import time\n",
    "import torch  # type: ignore\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(2)\n",
    "\n",
    "sets = [\n",
    "    {\"task\": \"mmlu\", \"enforce_eager\": False, \"gpu_memory_utilization\": 0.6},\n",
    "    {\"task\": \"wikitext\", \"enforce_eager\": True, \"gpu_memory_utilization\": 0.4},\n",
    "]\n",
    "\n",
    "set_0 = sets[1]\n",
    "model_name = \"gemma-2-9b-it-GPTQ\"\n",
    "quantization = \"gptq\"\n",
    "\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "# torch.cuda.set_device(1)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# torch.cuda.set_device(1)\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"basic_vllm.ipynb\"\n",
    "\n",
    "res = eval.eval(\n",
    "    model_name=model_name,\n",
    "    enforce_eager=set_0[\"enforce_eager\"],\n",
    "    tensor_parallel=False,\n",
    "    tasks=set_0[\"task\"],\n",
    "    # device=\"cuda:1\",\n",
    "    gpu_memory_utilization=set_0[\"gpu_memory_utilization\"],\n",
    "    max_model_len=2048,\n",
    "    quantization=quantization,\n",
    "    infer_dtye=\"float16\",\n",
    "    # kv_cache_dtype=\"fp8\",\n",
    "    # use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'gsm8k': {'alias': 'gsm8k',\n",
       "   'exact_match,strict-match': 0.7065959059893859,\n",
       "   'exact_match_stderr,strict-match': 0.01254183081546149,\n",
       "   'exact_match,flexible-extract': 0.7210007581501138,\n",
       "   'exact_match_stderr,flexible-extract': 0.012354115779970322}},\n",
       " 'group_subtasks': {'gsm8k': []},\n",
       " 'configs': {'gsm8k': {'task': 'gsm8k',\n",
       "   'tag': ['math_word_problems'],\n",
       "   'dataset_path': 'gsm8k',\n",
       "   'dataset_name': 'main',\n",
       "   'training_split': 'train',\n",
       "   'test_split': 'test',\n",
       "   'fewshot_split': 'train',\n",
       "   'doc_to_text': 'Question: {{question}}\\nAnswer:',\n",
       "   'doc_to_target': '{{answer}}',\n",
       "   'description': '',\n",
       "   'target_delimiter': ' ',\n",
       "   'fewshot_delimiter': '\\n\\n',\n",
       "   'num_fewshot': 5,\n",
       "   'metric_list': [{'metric': 'exact_match',\n",
       "     'aggregation': 'mean',\n",
       "     'higher_is_better': True,\n",
       "     'ignore_case': True,\n",
       "     'ignore_punctuation': False,\n",
       "     'regexes_to_ignore': [',', '\\\\$', '(?s).*#### ', '\\\\.$']}],\n",
       "   'output_type': 'generate_until',\n",
       "   'generation_kwargs': {'until': ['Question:', '</s>', '<|im_end|>'],\n",
       "    'do_sample': False,\n",
       "    'temperature': 0.0},\n",
       "   'repeats': 1,\n",
       "   'filter_list': [{'name': 'strict-match',\n",
       "     'filter': [{'function': 'regex',\n",
       "       'regex_pattern': '#### (\\\\-?[0-9\\\\.\\\\,]+)'},\n",
       "      {'function': 'take_first'}]},\n",
       "    {'name': 'flexible-extract',\n",
       "     'filter': [{'function': 'regex',\n",
       "       'group_select': -1,\n",
       "       'regex_pattern': '(-?[$0-9.,]{2,})|(-?[0-9]+)'},\n",
       "      {'function': 'take_first'}]}],\n",
       "   'should_decontaminate': False,\n",
       "   'metadata': {'version': 3.0}}},\n",
       " 'versions': {'gsm8k': 3.0},\n",
       " 'n-shot': {'gsm8k': 5},\n",
       " 'higher_is_better': {'gsm8k': {'exact_match': True}},\n",
       " 'n-samples': {'gsm8k': {'original': 1319, 'effective': 1319}},\n",
       " 'config': {'model': 'vllm',\n",
       "  'model_args': 'pretrained=/data/llmQuantModels/phi-3-mini-AWQ,dtype=auto,gpu_memory_utilization=0.4,batch_size=auto,enforce_eager=True,quantization=AWQ,',\n",
       "  'batch_size': None,\n",
       "  'batch_sizes': [],\n",
       "  'device': None,\n",
       "  'use_cache': None,\n",
       "  'limit': None,\n",
       "  'bootstrap_iters': 100000,\n",
       "  'gen_kwargs': None,\n",
       "  'random_seed': 0,\n",
       "  'numpy_seed': 1234,\n",
       "  'torch_seed': 1234,\n",
       "  'fewshot_seed': 1234},\n",
       " 'git_hash': '1e52658',\n",
       " 'date': 1721500413.0731828,\n",
       " 'pretty_env_info': 'PyTorch version: 2.3.0+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 20.04.6 LTS (x86_64)\\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\\nClang version: 10.0.0-4ubuntu1 \\nCMake version: version 3.30.0\\nLibc version: glibc-2.31\\n\\nPython version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0] (64-bit runtime)\\nPython platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31\\nIs CUDA available: True\\nCUDA runtime version: 12.1.66\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: \\nGPU 0: NVIDIA GeForce RTX 4090\\nGPU 1: NVIDIA GeForce RTX 4090\\n\\nNvidia driver version: 535.129.03\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.1\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.1\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.7\\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.7\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nByte Order:                         Little Endian\\nAddress sizes:                      46 bits physical, 57 bits virtual\\nCPU(s):                             64\\nOn-line CPU(s) list:                0-63\\nThread(s) per core:                 1\\nCore(s) per socket:                 32\\nSocket(s):                          2\\nNUMA node(s):                       2\\nVendor ID:                          GenuineIntel\\nCPU family:                         6\\nModel:                              106\\nModel name:                         Intel(R) Xeon(R) Platinum 8338C CPU @ 2.60GHz\\nStepping:                           6\\nCPU MHz:                            800.000\\nCPU max MHz:                        3500.0000\\nCPU min MHz:                        800.0000\\nBogoMIPS:                           5200.00\\nVirtualization:                     VT-x\\nL1d cache:                          3 MiB\\nL1i cache:                          2 MiB\\nL2 cache:                           80 MiB\\nL3 cache:                           96 MiB\\nNUMA node0 CPU(s):                  0-31\\nNUMA node1 CPU(s):                  32-63\\nVulnerability Gather data sampling: Mitigation; Microcode\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Not affected\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT disabled\\nVulnerability Retbleed:             Not affected\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Not affected\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.26.4\\n[pip3] torch==2.3.0\\n[pip3] torchvision==0.18.0\\n[pip3] triton==2.3.0\\n[conda] numpy                     1.26.4                   pypi_0    pypi\\n[conda] torch                     2.3.0                    pypi_0    pypi\\n[conda] torchvision               0.18.0                   pypi_0    pypi\\n[conda] triton                    2.3.0                    pypi_0    pypi',\n",
       " 'transformers_version': '4.42.4',\n",
       " 'upper_git_hash': None,\n",
       " 'tokenizer_pad_token': ['<|endoftext|>', '32000'],\n",
       " 'tokenizer_eos_token': ['<|endoftext|>', '32000'],\n",
       " 'tokenizer_bos_token': ['<s>', '1'],\n",
       " 'eot_token_id': 32000,\n",
       " 'max_length': 4096,\n",
       " 'model_size': None,\n",
       " 'avg_output_toks': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 4090\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "    torch.cuda.get_device_properties(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.944580078125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(16972316672 + 24844107776) / 1024 / 1024 / 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
